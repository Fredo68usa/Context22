# LSTM for international airline passengers problem with regression framing
import numpy as np
import matplotlib.pyplot as plt
from pandas import read_csv
import math
import json
import psutil
from elasticsearch import Elasticsearch, helpers
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error

class lstm :

    def __init__(self, param_jason):
       # print ("init")
       # self.es = Elasticsearch()
       with open(param_jason) as f:
          self.param_data = json.load(f)

       self.path = self.param_data["path"]
       self.pathlength = len(self.path)
       self.pathProcessed= self.param_data["pathProcessed"]
       self.esServer = self.param_data["ESServer"]
       self.esUser = self.param_data["ESUser"]
       self.esPwd = self.param_data["ESPwd"]
       self.esIndex = self.param_data["ESIndex"]
       # es = Elasticsearch(['http://localhost:9200'], http_auth=('user', 'pass'))
       self.es = Elasticsearch([self.esServer], http_auth=(self.esUser, self.esPwd))
       # print ("After connection to ES")

    # ---- getting Collectors MetaData
    def MemConsumption(self):
        # Getting % usage of virtual_memory ( 3rd field)
        print('RAM memory % used:', psutil.virtual_memory()[2])
        # Getting usage of virtual_memory in GB ( 4th field)
        print('RAM Used (GB):', psutil.virtual_memory()[3]/1000000000)

    def CPUConsumption(self):
        # Calling psutil.cpu_precent() for 4 seconds
        print('The CPU usage is: ', psutil.cpu_percent(4))

    def create_dataset(self,dataset, look_back=1):
        dataX, dataY = [], []
        for i in range(len(dataset)-look_back-1):
            a = dataset[i:(i+look_back), 0]
            dataX.append(a)
            dataY.append(dataset[i + look_back, 0])
        return np.array(dataX), np.array(dataY)

    def insert_many_Elastic(self,newPredictionSet):
      try:

           response = helpers.bulk(self.es,newPredictionSet, index=self.esIndex + '-20221032')
           print ("\nRESPONSE:", response)
           return(response)
      except Exception as e:
           print("\nERROR:", e)



    def TSData(self):
        print ("TSData")
        p1.CPUConsumption()
        p1.MemConsumption()

        # declare a filter query dict object
        match_all = {
            "size": 100,
            "query": {
                "match_all": {}
            }
           }
        # make a search() request to get all docs in the index
        resp = p1.es.search(
            index = self.esIndex,
            body = match_all,
            scroll = '2s' # length of time to keep search context
            )

        # keep track of pass scroll _id
        old_scroll_id = resp['_scroll_id']

        # keep track of the number of the documents returned
        doc_count = 0

        print ("old_scroll_id : " , old_scroll_id )

        myArrayTSpollution = []

        # use a 'while' iterator to loop over document 'hits'
        while len(resp['hits']['hits']):
            # make a request using the Scroll API
            resp = p1.es.scroll(
                scroll_id = old_scroll_id,
                scroll = '2s' # length of time to keep search context
            )

            # check if there's a new scroll ID
            if old_scroll_id != resp['_scroll_id']:
                print ("NEW SCROLL ID:", resp['_scroll_id'])

            # keep track of pass scroll _id
            old_scroll_id = resp['_scroll_id']

            # print the response results
            print ("\nresponse for index:", self.esIndex)
            print ("_scroll_id:", resp['_scroll_id'])
            print ('response["hits"]["total"]["value"]:', resp["hits"]["total"]["value"])

            # iterate over the document hits for each 'scroll'
            for doc in resp['hits']['hits']:
                print ("\n", doc['_id'], doc['_source'])
                myArrayTSpollution.append(doc["_source"]['TEMP'])
                doc_count += 1
                print ("DOC COUNT:", doc_count)

        # print the total time and document count at the end
        print ("\nTOTAL DOC COUNT:", doc_count)

        p1.MemConsumption()

        exit(0)

        myArrayTSairline = []
        for hit in TS_airline_tmp['hits']['hits']:
            myArrayTSairline.append(hit["_source"]['Thousands of Passengers'])
            # print (hit["_source"]['Month'], "--" ,hit["_source"]['Thousands of Passengers'])

        dataset = np.array(myArrayTSairline)
        dataset = dataset.reshape(-1, 1)
        # print (" My array" ,dataset)
        # print (" Its type " , type(dataset))
        # normalize the dataset
        scaler = MinMaxScaler(feature_range=(0, 1))
        dataset = scaler.fit_transform(dataset)
        # print (" dataset " , dataset)

        # fix random seed for reproducibility
        tf.random.set_seed(7)

        # split into train and test sets
        train_size = int(len(dataset) * 0.67)
        test_size = len(dataset) - train_size
        train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]
        # print(len(train), len(test))

        # reshape into X=t and Y=t+1
        look_back = 1
        trainX, trainY = p1.create_dataset(train, look_back)
        testX, testY = p1.create_dataset(test, look_back)
        # reshape input to be [samples, time steps, features]
        trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))
        testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))
        # print (testX)

        # create and fit the LSTM network
        model = Sequential()
        model.add(LSTM(4, input_shape=(1, look_back)))
        model.add(Dense(1))
        model.compile(loss='mean_squared_error', optimizer='adam')
        result = model.fit(trainX, trainY, epochs=100, batch_size=1, verbose=2) # Displays the results 
        # print(" Maybe a loss " , type(result))
        history = result
        # print ("history params" , history.params)
        # print(history.history.keys())

        # make predictions
        trainPredict = model.predict(trainX)
        # print (" trainPredict .." , len(trainPredict), trainPredict)
        testPredict = model.predict(testX)
        # print (" testPredict .." , len(testPredict),testPredict)
        # invert predictions
        trainPredict = scaler.inverse_transform(trainPredict)
        trainY = scaler.inverse_transform([trainY])
        testPredict = scaler.inverse_transform(testPredict)
        testY = scaler.inverse_transform([testY])
        # calculate root mean squared error
        trainScore = np.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
        # print('Train Score: %.2f RMSE' % (trainScore))
        testScore = np.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
        # print('Test Score: %.2f RMSE' % (testScore))
        # print (len(testY[0]), len(testPredict[:,0]))
        # print (testY[0], testPredict[:,0])

        # shift train predictions for plotting
        trainPredictPlot = np.empty_like(dataset)
        trainPredictPlot[:, :] = np.nan
        trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
        # print (trainPredictPlot)
        # shift test predictions for plotting
        testPredictPlot = np.empty_like(dataset)
        testPredictPlot[:, :] = np.nan
        testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict
        # print (len(testPredictPlot))
        # for predict in testPredictPlot:
        # for i in range(0,len(testPredictPlot)):
           # print (i,"---",testPredictPlot[i])
           # print (i,"---",TS_airline_tmp[i])
        docS = []
        i = 0
        for key in TS_airline_tmp['hits']['hits'] :
            # print ("key : ", key["_source"])
            doc= key["_source"]
            # print (i,"---",testPredictPlot[i][0])
            if math.isnan(testPredictPlot[i][0]) == False:
               doc['Pred']=testPredictPlot[i][0]
               # print('Pred',type(testPredictPlot[i][0]))
               # print("doc : ",doc)
               docS.append(doc)
            i = i + 1

        response = p1.insert_many_Elastic(docS)
        # print ("Response After call : ",response)


# --- Main  ---
if __name__ == '__main__':
    print("Start LSTM Pollution")

    p1 = lstm("param_data.json")

    p1.TSData()

    print("End of LSTM Pollution")
